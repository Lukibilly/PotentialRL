{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First RL Gym Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from environments import BoxEnvironment1 as env\n",
    "from environment_utils import Box\n",
    "from agents import SACAgent\n",
    "from agent_utils import update_target_agent, ReplayBuffer\n",
    "from log_utils import RLLogger\n",
    "from plot_utils import RLPlotter, plot_normalized_mexican_hat_potential\n",
    "\n",
    "device = tr.device('cuda' if tr.cuda.is_available() else 'cpu')\n",
    "tr.autograd.set_detect_anomaly(True)\n",
    "tr.set_default_tensor_type(tr.FloatTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Training -------------------\n",
    "    # Memory\n",
    "memory_size = 800\n",
    "memory_batch_size = 32\n",
    "    # Duration of training\n",
    "runs = 1\n",
    "n_episodes = 20\n",
    "n_steps = 128\n",
    "    # Training parameters\n",
    "agent_batch_size = 1\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.8\n",
    "entropy_coef = 0.2\n",
    "    # Bellman equation\n",
    "future_discount = 0.8\n",
    "    # Update Target Model\n",
    "target_model_update = 16\n",
    "    # Loss Function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# ---------------- Environment  ----------------\n",
    "    # Environment box size\n",
    "env_width = 2\n",
    "env_height = 2\n",
    "space = Box(env_width, env_height)\n",
    "    # Goal box size and center\n",
    "goal_width = 0.2\n",
    "goal_height = 0.2\n",
    "goal_center = np.tile([0.5,0],(agent_batch_size,1))\n",
    "goal = Box(goal_width, goal_height, goal_center)\n",
    "    # Time step size\n",
    "dt = 0.0375\n",
    "    # Noise\n",
    "noise_characteristic_length = 1\n",
    "    # Maximum of potential\n",
    "U0 = 0\n",
    "\n",
    "# ---------------- Agent ----------------------\n",
    "state_dim = 4\n",
    "hidden_dims = [16,16]\n",
    "act_dim = 1\n",
    "act_positive = True\n",
    "act_scaling = 2*np.pi\n",
    "\n",
    "# ---------------- Other ----------------------\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "plt.rcParams.update({'figure.dpi': 150})\n",
    "total_time = []\n",
    "update_state_time = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env(space, goal)\n",
    "memory = ReplayBuffer(state_dim, act_dim, memory_size, agent_batch_size)\n",
    "agent = SACAgent(state_dim, act_dim, hidden_dims, act_scaling, act_positive).float().to(device)\n",
    "target_agent = SACAgent(state_dim, act_dim, hidden_dims, act_scaling, act_positive).float().to(device)\n",
    "logger = RLLogger()\n",
    "plotter = RLPlotter(logger)\n",
    "\n",
    "agent.actor_optimizer = tr.optim.Adam(agent.actor.parameters(), lr=learning_rate)\n",
    "agent.critic1_optimizer = tr.optim.Adam(agent.critic1.parameters(), lr=learning_rate)\n",
    "agent.critic2_optimizer = tr.optim.Adam(agent.critic2.parameters(), lr=learning_rate)\n",
    "\n",
    "for p in target_agent.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, target_agent, memory_batch):\n",
    "    state_now = memory_batch['state_now']\n",
    "    state_next = memory_batch['state_next']\n",
    "    action_now = memory_batch['action_now']\n",
    "    reward = memory_batch['reward']\n",
    "    done = memory_batch['done']\n",
    "    action_next, log_prob_next = agent.actor(state_next)\n",
    "    \n",
    "    # Compute Prediction\n",
    "    Q1_now = agent.critic1(state_now, action_now)\n",
    "    Q2_now = agent.critic2(state_now, action_now)\n",
    "    Q_now = tr.min(Q1_now, Q2_now)\n",
    "\n",
    "    # Compute Target\n",
    "    Q1_next = target_agent.critic1(state_next, action_next)\n",
    "    Q2_next = target_agent.critic2(state_next, action_next)\n",
    "    Q_next = tr.min(Q1_next, Q2_next)\n",
    "    Target = reward + future_discount*(Q_next - entropy_coef*log_prob_next)\n",
    "     \n",
    "    # Compute Loss\n",
    "    loss = loss_function(Q_now, Target)\n",
    "    \n",
    "    # Update\n",
    "    loss.backward()\n",
    "    agent.critic1_optimizer.step()\n",
    "    agent.critic2_optimizer.step()\n",
    "    agent.actor_optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode():    \n",
    "    environment.init_state(agent_batch_size, state_dim)\n",
    "    for current_step in range(n_steps):\n",
    "        logger.save_state(environment.state)\n",
    "        if current_step%target_model_update == 0 and current_step > memory_size:\n",
    "            update_target_agent(agent, target_agent)\n",
    "        # Beginning state\n",
    "        state_now = environment.state\n",
    "        # Action\n",
    "        action_now, _ = agent.actor(tr.as_tensor(environment.state, device=device, dtype=tr.float))\n",
    "        # Next state\n",
    "        reward = environment.step(action_now.detach().cpu().numpy(), U0, dt, noise_characteristic_length)\n",
    "        state_next = environment.state\n",
    "        # Done\n",
    "        done = environment.goal_check()\n",
    "        # Store in memory\n",
    "        memory.store(state_now, action_now, reward, state_next, done)\n",
    "\n",
    "        logger.save_action(action_now.detach().cpu().numpy())\n",
    "\n",
    "        # Sample from memory\n",
    "        if memory.size >= memory_batch_size:\n",
    "            memory_batch = memory.sample_batch(memory_batch_size)\n",
    "            # Update Agent\n",
    "            loss = update(agent, target_agent, memory_batch)\n",
    "            logger.save_loss(loss.item())\n",
    "\n",
    "        if max(environment.goal_check()): \n",
    "            print('Goal reached')\n",
    "            logger.save_state(environment.state)\n",
    "            break\n",
    "        \n",
    "    return current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0  finished!\n",
      "Episode 1  finished!\n",
      "Episode 2  finished!\n",
      "Episode 3  finished!\n",
      "Episode 4  finished!\n",
      "Episode 5  finished!\n",
      "Episode 6  finished!\n",
      "Episode 7  finished!\n",
      "Episode 8  finished!\n",
      "Episode 9  finished!\n",
      "Episode 10  finished!\n",
      "Episode 11  finished!\n",
      "Episode 12  finished!\n",
      "Episode 13  finished!\n",
      "Episode 14  finished!\n",
      "Episode 15  finished!\n",
      "Episode 16  finished!\n",
      "Episode 17  finished!\n",
      "Episode 18  finished!\n",
      "Goal reached\n",
      "Episode 19  finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 960x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simulation():\n",
    "    update_target_agent(agent, target_agent)\n",
    "    for ep in range(n_episodes):\n",
    "        episode_steps = episode()\n",
    "        logger.save_episode(episode_steps)\n",
    "        plotter.plot_last_episode()\n",
    "        print('Episode', ep,' finished!')\n",
    "\n",
    "plotter.clear_plots()\n",
    "simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
